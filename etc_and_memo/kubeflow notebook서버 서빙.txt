실행 환경: notebook server
스토리지: ??
어디 클러스터에서 실행되나?

notebook server는 클러스터에 파드로 띄워짐 -> kubectl exec -it $podname -n $namespace -- /bin/bash 로 접속 가능함.
노트북 서버에 mar파일 생성하는 torch model archiver 설치
pip install --upgrade pip
pip install torch-model-archiver==0.7.1 torch-workflow-archiver==0.2.7 torchserve==0.7.1
pip uninstall pyOpenSSL -y
pip uninstall kfserving -y
pip install kserve==0.7.0


이제 노트북서버에서 작업
# 서빙 관련 파일 다운 
git clone https://github.com/LukaMagics/kserve-pytorch.git

미리 mar파일 만들어놓기
torch-model-archiver --model-name torch2 --version 1.0 \
--model-file torch_iris.py \
--serialized-file model.pt \
--handler torch_iris_handler.py

디렉토리 구조 이렇게해서
kserve-pytorch
├── config
│   ├── config.properties
├── model-store
│   ├── torch2.mar  # 코드 변경 시 삭제 후 model archiver 명령으로 재생성
│   ├── torch_iris.py # 모델 파일
│   ├── torch_iris_handler.py # 핸들러 파일
│   ├── model.pt # 훈련된 모델 state dict 파일

# config.properties 파일
inference_address=http://0.0.0.0:8085
management_address=http://0.0.0.0:8085
metrics_address=http://0.0.0.0:8082
grpc_inference_port=7070
grpc_management_port=7071
enable_metrics_api=true
metrics_format=prometheus
number_of_netty_threads=4
job_queue_size=10
enable_envvars_config=true
install_py_dep_per_model=true
model_store=/mnt/models/model-store
model_snapshot={"name":"startup.cfg","modelCount":1,"models":{"torch2":{"1.0":{"defaultVersion":true,"marName":"torch2.mar","minWorkers":1,"maxWorkers":5,"batchSize":1,"maxBatchDelay":5000,"responseTimeout":120}}}}

★model_snapshot안에 파라미터 하나라도 빠지면 서빙 안됨

# jupyter kserve 서빙 코드
from kubernetes import client 
from kserve import KServeClient
from kserve import constants
from kserve import utils
from kserve import V1beta1InferenceService
from kserve import V1beta1InferenceServiceSpec
from kserve import V1beta1PredictorSpec
from kserve import V1beta1SKLearnSpec

namespace = utils.get_default_target_namespace()

name='torch-iris'  # 여기서 name을 config.properties에 모델명과 일치 시켜야 함(X)
kserve_version='v1beta1'
api_version = constants.KSERVE_GROUP + '/' + kserve_version

isvc = V1beta1InferenceService(api_version=api_version,
                               kind=constants.KSERVE_KIND,
                               metadata=client.V1ObjectMeta(
                                   name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),
                               spec=V1beta1InferenceServiceSpec(
                               predictor=V1beta1PredictorSpec(
                               pytorch=(V1beta1SKLearnSpec(
                                   storage_uri="pvc://user1-volume/kserve-pytorch"))))
)

KServe = KServeClient()
KServe.create(isvc)

KServe.get(name, namespace=namespace, watch=True, timeout_seconds=120)

# kserve 서빙 서버 관련

상태 진단
k get pods -n kubeflow-user-example-com

kubectl logs -f torch2-predictor-default-00001-deployment-65d4f7c968-x59v8 -c kserve-container -n kubeflow-user-example-com
kubectl logs -f torch2-predictor-default-00001-deployment-5fb4b4c64-kvj69 -c queue-proxy -n kubeflow-user-example-com
kubectl logs -f torch2-predictor-default-00001-deployment-5fb4b4c64-kvj69 -c storage-initializer -n kubeflow-user-example-com

kubectl describe deployment torch-iris-predictor-default-00001-deployment -n kubeflow-user-example-com

BASH 접속
kubectl exec -it torch2-predictor-default-00001-deployment-5fb4b4c64-kvj69 -c kserve-container -n kubeflow-user-example-com -- bash
>>> queue proxy랑 storage initializaer는 위와 같이 bash로 접속 못하는듯

# yaml 출력
k get pod $podname -n kubeflow-user-example-com -o yaml

file:///home/jovyan

삭제 시
kubectl get inferenceservice -n kubeflow-user-example-com
kubectl delete inferenceservice torch2 -n kubeflow-user-example-com

# kserve 서빙 deployment yaml 수정
kubectl edit deployment torch-iris-predictor-default-00001-deployment -n kubeflow-user-example-com
(edit 명령어로 수정 시 자동 반영)

# pvc, pv 전체 목록 확인
kubectl get pvc -A
kubectl get pv -A
# pvc yaml 파일 출력
kubectl get pvc user1-volume -n kubeflow-user-example-com -o yaml
# pv yaml 파일 >>> host path 확인
kubectl get pv pvc-d435ed54-fc0e-43dd-88aa-740f1010759c -n kubeflow-user-example-com -o yaml
# host path 정보
/opt/local-path-provisioner/pvc-d435ed54-fc0e-43dd-88aa-740f1010759c_kubeflow-user-example-com_user1-volume
/opt/local-path-provisioner/pvc-d435ed54-fc0e-43dd-88aa-740f1010759c_kubeflow-user-example-com_user1-volume/model-store
/opt/local-path-provisioner/pvc-d435ed54-fc0e-43dd-88aa-740f1010759c_kubeflow-user-example-com_user1-volume/config

# inference 요청
predict

1A. Dex 인증을 위한 토큰 발행(토큰은 24시간 유효)

## python 코드
import requests
HOST = "http://192.168.0.5:30370"
USERNAME = "user@example.com"
PASSWORD = "12341234"
session = requests.Session()
response = session.get(HOST)
headers = {
    "Content-Type": "application/x-www-form-urlencoded",
}
data = {"login": USERNAME, "password": PASSWORD}
session.post(response.url, headers=headers, data=data)
session_cookie = session.cookies.get_dict()["authservice_session"]
print(session_cookie)

or 브라우저 검사 도구에서 token값 복사

1B. DEX 인증을 우회 ( https://1week.tistory.com/83 )
공통 적용되는(All virtual host) dex 정책의 yaml 파일은 아래 명령어로 확인
k get envoyfilters.networking.istio.io authn-filter -n istio-system -o yaml

## 우회하는 filter 오브젝트를 추가 적용해야 함
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"networking.istio.io/v1alpha3","kind":"EnvoyFilter","metadata":{"annotations":{},"name":"bypass-auth-torch2","namespace":"istio-system"},"spec":{"configPatches":[{"applyTo":"VIRTUAL_HOST","match":{"routeConfiguration":{"vhost":{"name":"192.168.0.5:30370/v2/models/torch2/infer"}}},"patch":{"operation":"MERGE","value":{"per_filter_config":{"envoy.ext_authz":{"disabled":true}}}}}],"workloadSelector":{"labels":{"istio":"ingressgateway"}}}}
  creationTimestamp: "2023-04-07T05:07:01Z"
  generation: 2
  name: bypass-auth-torch2
  namespace: istio-system
  resourceVersion: "1164798"
  uid: 8fd0664d-7bd6-48a7-ac7f-b1d530337e4e
spec:
  configPatches:
  - applyTo: VIRTUAL_HOST
    match:
      routeConfiguration:
        vhost:
          name: http://192.168.0.5:30370/v2/models/torch2/infer
    patch:
      operation: MERGE
      value:
        per_filter_config:
          envoy.ext_authz:
            disabled: true
  workloadSelector:
    labels:
      istio: ingressgateway

# yaml 적용
kubectl apply -f bypass-auth-torch2.yaml

# 적용됐는지 확인
k get envoyfilters.networking.istio.io -n istio-system

2. BASH 창을 통해 curl

TOKEN=MTY4MDg0MDY4N3xOd3dBTkRVelNsWllTa2hEUjBGQlJsaFZWMFJJUzFKSU1rMUxNMEZWUjA1VlYxRk1UekpRVVZORVVWcElSVmxEVFVWV1drbENSMEU9fNOWzqG1qryUErGeGuKhuIrY84GPKufc8CF7uxmM3ZYm
SERVICE_HOSTNAME=torch2.kubeflow-user-example-com.example.com

## json 데이터로 넣을 때
INPUT_DATA='{"instances": [[6.8, 2.8, 4.8, 1.4], [5.1, 3.5, 1.4, 0.2], [6.3, 2.5, 5.0, 1.9]]}'
curl -v 192.168.0.5:30370/v2/models/torch2/infer -d "${INPUT_DATA}" -H "Cookie: authservice_session=${TOKEN}" -H "Host: ${SERVICE_HOSTNAME}"

## json 파일 넣을 때
curl -v 192.168.0.5:30370/v2/models/torch2/infer -d @iris-input.json -H "Cookie: authservice_session=${TOKEN}" -H "Host: ${SERVICE_HOSTNAME}"
curl -v 192.168.0.5:30370/v2/models/torch2/infer -T iris-input.json -H "Cookie: authservice_session=${TOKEN}" -H "Host: ${SERVICE_HOSTNAME}"

curl -v 192.168.0.5:30370/v2/models/torch2/infer -d "${INPUT_DATA}" -H "Host: ${SERVICE_HOSTNAME}"

#  dex에 관한 버추얼 서비스와 이 서비스가 사용하는 게이트웨이 확인
kubectl get virtualservices.networking.istio.io --all-namespaces

# kubeflow 게이트웨이의 정보를 확인
kubectl get gateways.networking.istio.io -n kubeflow kubeflow-gateway -o yaml
>>> kubeflow gateway를 이용하는 오브젝트는 모두 istio: ingressgateway가 selector로 지정된 것을 알 수 있음

# 참고사항

1. 2021년 9월 기준 Currently Torhcserve KFServing does not support model management via REST APIs

2. 
서빙 시 올라가는 파드에 kserve-container라는 컨테이너에는 아래 경로가 생성됨
/mnt/pvc/[storage_uri에 지정한 pvc의 working dir]
/mnt/models/[storage_uri 경로 또는 config.properties에 model-store]
여기서 mar파일 기반으로 /home/model-server/tmp/${uid}/ 경로에 handler.py, model.py, pt파일가 생성되고 inference 시 이걸 실행함

3.
torchserve와 다르게 JSON파일(아래 iris-input.json)을 input하면 handler에는 value 부분만 들어감
# original input
{
  "instances": [
    [6.8,  2.8,  4.8,  1.4],
    [5.1,  3.5,  1.4,  0.2],
    [6.3,  2.5,  5.0,  1.9]
  ]
}
# handler input
[[6.8, 2.8, 4.8, 1.4], [5.1, 3.5, 1.4, 0.2], [6.3, 2.5, 5.0, 1.9]] 

4.
inferenceservice올리면서 kserve-container 올리가는데 여기에서 pvc에 있는 handler코드나 mar파일 바꾸면 kserve-container에 /mnt/pvc나 /mnt/models 쪽 파일은 변경되지만 mar 파일 기반으로 만들어진 /home/model-store/tmp 안에 handler는 안바뀌기 때문에 결국 다시 서빙 서버 올려야 하는듯함...